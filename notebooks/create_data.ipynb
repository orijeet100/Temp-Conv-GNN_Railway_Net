{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:17:23.319469900Z",
     "start_time": "2024-04-03T05:17:23.305971400Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/55/d1/a3631a36859ee324e1767fa7554fdf7af17965571d8537b20b311b76bcfe/tensorflow-2.11.0-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.11.0-cp37-cp37m-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting tensorflow-intel==2.11.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.11.0 from https://files.pythonhosted.org/packages/f7/8c/18288ac12dc0e1997c73f1b86dbd6f7fa3674ae5341769387e1f13b07c9e/tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=2.0 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for gast<=0.4.0,>=0.2.1 from https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl.metadata\n",
      "  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/44/c6/ebb183fcd341681fe41d86b10790335c27372dd279189ab514c75acdbfb2/h5py-3.8.0-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached h5py-3.8.0-cp37-cp37m-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/0b/2d/3f480b1e1d31eb3d6de5e3ef641954e5c67430d5ac93b7fa7e07589576c7/libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\raildelays-public\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.6)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in d:\\raildelays-public\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (24.0)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for protobuf<3.20,>=3.9.2 from https://files.pythonhosted.org/packages/70/ee/e3562fd4e692afc6ed396b60ce3a177bc4ce6506ac8ac2413886198880e3/protobuf-3.19.6-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached protobuf-3.19.6-cp37-cp37m-win_amd64.whl.metadata (807 bytes)\n",
      "Requirement already satisfied: setuptools in d:\\raildelays-public\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\raildelays-public\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/67/e1/434566ffce04448192369c1a282931cf4ae593e91907558eaecd2e9f2801/termcolor-2.3.0-py3-none-any.whl.metadata\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\raildelays-public\\venv\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.7.1)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for wrapt>=1.11.0 from https://files.pythonhosted.org/packages/66/a5/50e6a2bd4cbf6671012771ec35085807a375da5e61540bc5f62de62ba955/wrapt-1.16.0-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached wrapt-1.16.0-cp37-cp37m-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/99/fb/8354f56519f318aca100a7390a52ef90d7bd1031e31b42fa0a68992afd69/grpcio-1.62.1-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached grpcio-1.62.1-cp37-cp37m-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.12,>=2.11 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.12,>=2.11 from https://files.pythonhosted.org/packages/6f/77/e624b4916531721e674aa105151ffa5223fb224d3ca4bd5c10574664f944/tensorboard-2.11.2-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.12,>=2.11.0 from https://files.pythonhosted.org/packages/bb/e2/8bf618c7c30a525054230ee6d40b036d3e5abc2c4ff67cf7c7420a519204/tensorflow_estimator-2.11.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.12,>=2.11.0 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.12,>=2.11.0 from https://files.pythonhosted.org/packages/de/44/bf1b0eef5b13e6201aef076ff34b91bc40aace8591cd273c1c2a94a9cc00/keras-2.11.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/3f/b1/1702140b92543318ecca7ed502fcb18085a9d5760058cf9ea2310bb8dbd6/tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\raildelays-public\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.41.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9e/8d/ddbcf81ec751d8ee5fd18ac11ff38a0e110f39dfbf105e6d9db69d556dd0/google_auth-2.29.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth-oauthlib<0.5,>=0.4.1 from https://files.pythonhosted.org/packages/b1/0e/0636cc1448a7abc444fb1b3a63655e294e0d2d49092dc3de05241be6d43c/google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Using cached Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for requests<3,>=2.21.0 from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.7.0,>=0.6.0 from https://files.pythonhosted.org/packages/74/69/5747a957f95e2e1d252ca41476ae40ce79d70d38151d2e494feb7722860c/tensorboard_data_server-0.6.1-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-plugin-wit>=1.6.0 from https://files.pythonhosted.org/packages/e0/68/e8ecfac5dd594b676c23a7f07ea34c197d7d69b3313afdf8ac1b0a9905a2/tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/f6/f8/9da63c1617ae2a1dec2fbf6412f3a0cfe9d4ce029eccbda6e1e4258ca45f/Werkzeug-2.2.3-py3-none-any.whl.metadata\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/fb/2b/a64c2d25a37aeb921fddb929111413049fc5f8b9a4c1aefaffaafe768d54/cachetools-5.3.3-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for pyasn1-modules>=0.2.1 from https://files.pythonhosted.org/packages/cd/8e/bea464350e1b8c6ed0da3a312659cb648804a08af6cacc6435867f74f8bd/pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for requests-oauthlib>=0.7.0 from https://files.pythonhosted.org/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\raildelays-public\\venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (6.7.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/96/fc/0cae31c0f150cd1205a2a208079de865f69a8fd052a98856c40c99e36b3c/charset_normalizer-3.3.2-cp37-cp37m-win_amd64.whl.metadata\n",
      "  Using cached charset_normalizer-3.3.2-cp37-cp37m-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\raildelays-public\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.6)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/d2/b2/b157855192a68541a91ba7b2bbcb91f1b4faa51f8bae38d8005c034be524/urllib3-2.0.7-py3-none-any.whl.metadata\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/ba/06/a07f096c664aeb9f01624f858c3add0a4e913d6c96257acb4fce61e7de14/certifi-2024.2.2-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\raildelays-public\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\raildelays-public\\venv\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.15.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for pyasn1<0.6.0,>=0.4.6 from https://files.pythonhosted.org/packages/d1/75/4686d2872bf2fc0b37917cbc8bbf0dd3a5cdb0990799be1b9cbf1e1eb733/pyasn1-0.5.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow)\n",
      "  Obtaining dependency information for oauthlib>=3.0.0 from https://files.pythonhosted.org/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl.metadata\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Using cached tensorflow-2.11.0-cp37-cp37m-win_amd64.whl (1.9 kB)\n",
      "Using cached tensorflow_intel-2.11.0-cp37-cp37m-win_amd64.whl (266.3 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.62.1-cp37-cp37m-win_amd64.whl (4.6 MB)\n",
      "Using cached h5py-3.8.0-cp37-cp37m-win_amd64.whl (2.6 MB)\n",
      "Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-3.19.6-cp37-cp37m-win_amd64.whl (896 kB)\n",
      "Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Using cached wrapt-1.16.0-cp37-cp37m-win_amd64.whl (37 kB)\n",
      "Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Using cached Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp37-cp37m-win_amd64.whl (98 kB)\n",
      "Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, wrapt, werkzeug, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, keras, h5py, grpcio, google-pasta, gast, charset-normalizer, certifi, cachetools, astunparse, absl-py, rsa, requests, pyasn1-modules, markdown, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 flatbuffers-24.3.25 gast-0.4.0 google-auth-2.29.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.8.0 keras-2.11.0 libclang-18.1.1 markdown-3.4.4 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 urllib3-2.0.7 werkzeug-2.2.3 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:25:51.030660500Z",
     "start_time": "2024-04-03T05:24:30.453980200Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T05:27:38.550704500Z",
     "start_time": "2024-04-03T05:27:36.496419500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\raildelays-public\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OrderedDict' from 'typing' (C:\\Users\\Orijeet_Mukherjee\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_25796\\357146472.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;31m# custom libraries\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0msrc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mz_score\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msrc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_processing\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdata_interface\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\src\\utils\\utils.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpdb\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdatetime\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtyping\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_typing\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtools\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodule_util\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_module_util\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlazy_loader\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLazyLoader\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0m_LazyLoader\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;31m# Bring in subpackages.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdistribute\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[1;31m# from tensorflow.python import keras\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;31m# pylint: disable=unused-import\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mexperimental\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset_ops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAUTOTUNE\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset_ops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[1;31m# pylint: disable=unused-import\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 96\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mservice\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     97\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatching\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdense_to_ragged_batch\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatching\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdense_to_sparse_batch\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    417\u001B[0m \"\"\"\n\u001B[0;32m    418\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 419\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_service_ops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdistribute\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    420\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_service_ops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfrom_dataset_id\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    421\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata_service_ops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mregister_dataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mservice\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_pywrap_server_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mservice\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_pywrap_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdataset_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0moptions\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0moptions_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mstructured_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mgraph_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtf2\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0miterator_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0moptions\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0moptions_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mstructured_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mops\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mgen_dataset_ops\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtrackable\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 34\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraining\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaver\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mBaseSaverBuilder\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     35\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0m_pywrap_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdeprecation\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msaver_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrackable_object_graph_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 32\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckpoint\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcheckpoint_management\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     33\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclient\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\checkpoint\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;34m\"\"\"API defining checkpoint.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckpoint\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcheckpoint_view\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint_view.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrackable_object_graph_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheckpoint\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrackable_view\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0merrors_impl\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplatform\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtf_logging\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mlogging\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\checkpoint\\trackable_view.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mconverter\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mobject_identity\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtf_export\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtf_export\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\trackable\\converter.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msaved_model_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtensor_util\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\saved_model_utils.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0masset\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtrackable\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 36\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mresource\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\trackable\\resource.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdef_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrackable\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mbase\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;31m# Config Options\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mset_dynamic_variable_creation\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mrun_functions_eagerly\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfunctions_run_eagerly\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mlift_to_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmonitoring\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 76\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfunction_spec\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfunction_spec_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     77\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmonomorphic_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtracing_compiler\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\function_spec.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrace_type\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphism\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfunction_type\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfunction_type_lib\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meager\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolymorphic_function\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcomposite_tensor_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcomposite_tensor\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\raildelays-public\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcollections\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtyping\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCallable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mMapping\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mSequence\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mTuple\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mOrderedDict\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrace_type\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'OrderedDict' from 'typing' (C:\\Users\\Orijeet_Mukherjee\\AppData\\Local\\Programs\\Python\\Python37\\lib\\typing.py)"
     ]
    }
   ],
   "source": [
    "# external libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = r\"D:\\raildelays-public\"\n",
    "\n",
    "# change directory for loading files from disk and loading custom dictionaries\n",
    "sys.path.append(base_dir)\n",
    "os.chdir(base_dir)\n",
    "print(os.getcwd())\n",
    "\n",
    "# custom libraries\n",
    "from src.utils.utils import z_score\n",
    "from src.data.data_processing import data_interface\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:32.381152Z",
     "start_time": "2020-03-24T17:23:32.377734Z"
    }
   },
   "outputs": [],
   "source": [
    "# set global options\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "figsize = (5, 4)\n",
    "dpi = 300\n",
    "node_size = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:32.532788Z",
     "start_time": "2020-03-24T17:23:32.526171Z"
    }
   },
   "outputs": [],
   "source": [
    "# include all these data points\n",
    "raw_dir = \"./data/raw/\"\n",
    "interim_dir = \"./data/interim/\"\n",
    "processed_dir = \"./data/processed/\"\n",
    "Path(raw_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(interim_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(processed_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# input data locations\n",
    "raw_data_path_2016 = raw_dir + \"HSP_2016_DID_PAD.txt\"\n",
    "raw_data_path_2017 = raw_dir + \"HSP_2017_DID_PAD.txt\"\n",
    "stop_location_path = raw_dir + \"stations.csv\"\n",
    "\n",
    "# save locations\n",
    "adjlist_path = interim_dir + \"adjlist.txt\"\n",
    "adj_path = processed_dir + \"adj.npy\"\n",
    "dataset_path = processed_dir + \"dataset.npy\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link-Based Node Formulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:42.117861Z",
     "start_time": "2020-03-24T17:23:33.684566Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(raw_data_path_2016, sep=\",\")\n",
    "df2 = pd.read_csv(raw_data_path_2017, sep=\",\")\n",
    "df2 = df2.drop([\"Unnamed: 0\"], axis=1)\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "# 2017 includes XXXPAD and PADXXX trips, 2016 does not, so remove the outbound trips\n",
    "df = df[df[\"station_origin\"] != \"PAD\"]\n",
    "\n",
    "# remove any stops that are included in either year but not the other\n",
    "stops_2016 = np.unique(df1[\"station_curr\"])\n",
    "stops_2017 = np.unique(df2[\"station_curr\"])\n",
    "stops_drop = []\n",
    "\n",
    "for i in stops_2017:\n",
    "    if i not in stops_2016:\n",
    "        stops_drop.append(i)\n",
    "for i in stops_2016:\n",
    "    if i not in stops_2017:\n",
    "        stops_drop.append(i)\n",
    "for i in stops_drop:\n",
    "    df = df[df[\"station_curr\"] != i]\n",
    "    \n",
    "\n",
    "# remove stations that serve an average of less than stop_thres trains per day \n",
    "stations = np.unique(df[\"station_curr\"])\n",
    "n_days = len(np.unique(df[\"date\"]))\n",
    "# stops per station\n",
    "sps = dict(collections.Counter(df[\"station_curr\"]))\n",
    "sps = dict(sorted(sps.items()))\n",
    "\n",
    "# if you change this value you will have to manually change the corridors and available routes since\n",
    "# that process is difficult to automate at this point in time\n",
    "stop_thres = 1\n",
    "counter = 0\n",
    "for i in list(sps):\n",
    "    if (sps[i] / n_days) < stop_thres:\n",
    "        # find set of RID's associated with these\n",
    "        df_tmp = df.loc[df[\"station_curr\"] == i]\n",
    "        RID_drop = np.unique(df_tmp[\"RID\"])\n",
    "        # remove train routes that stop at any of the non-included stations\n",
    "        for j in RID_drop:\n",
    "            df = df.loc[df[\"RID\"] != j]\n",
    "        sps.pop(i)\n",
    "\n",
    "df = df.reset_index()\n",
    "df = df.drop([\"index\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:42.189991Z",
     "start_time": "2020-03-24T17:23:42.119054Z"
    }
   },
   "outputs": [],
   "source": [
    "stops_data_period = np.unique(df[\"station_curr\"])\n",
    "df_sp = pd.read_csv(stop_location_path)\n",
    "G_stop2location = {}\n",
    "for i in range(len(df_sp)):\n",
    "    curr_row = df_sp[i:i+1]\n",
    "    if curr_row[\"crs\"].item() in stops_data_period:\n",
    "        G_stop2location[curr_row[\"crs\"].item()] = (curr_row[\"lat\"].item(), curr_row[\"lon\"].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:42.200399Z",
     "start_time": "2020-03-24T17:23:42.191524Z"
    }
   },
   "outputs": [],
   "source": [
    "# all corridors are defined in linear order\n",
    "## once a train gets on a corridor, it travels through all the stations on that corridor\n",
    "## although it does not necessarily stop at any of the stations on that corridor\n",
    "\n",
    "# the corridors are designed with the threshold of at least 1 train per day on average\n",
    "## it keeps out the \"end of line\" stations which just add noise to the data\n",
    "## in total this removes ~1% of the data\n",
    "\n",
    "# list of railway stations\n",
    "## https://en.wikipedia.org/wiki/UK_railway_stations_%E2%80%93_A\n",
    "\n",
    "# station map for defining corridors\n",
    "## https://www.gwr.com/plan-journey/stations-and-routes \n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# Northwest Corridor\n",
    "# --------------------------------------------\n",
    "# SWA - SWANSEA - START OF CORRIDOR\n",
    "# NTH - NEATH\n",
    "# PTA - PORT TALBOT PARKWAY\n",
    "# BGN - BRIDGEND\n",
    "# CDF - CARDIFF CENTRAL\n",
    "# NWP - NEWPORT\n",
    "# BPW - BRISTOL PARKWAY\n",
    "# SWI - SWINDON - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Southwest Corridor\n",
    "# --------------------------------------------\n",
    "# WSM - WESTON-SUPER-MARE - START OF CORRIDOR\n",
    "# WNM - WESTON MILTON\n",
    "# WOR - WORLE\n",
    "# YAT - YATTON\n",
    "# NLS - NAILSEA AND BLACKWELL\n",
    "# BRI - BRISTON TEMPLE MEADS\n",
    "# BTH - BATH SPA\n",
    "# CPM - CHIPPENHAM\n",
    "# SWI - SWINDON - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Middle Connecting Corridor\n",
    "# --------------------------------------------\n",
    "# SWI - SWINDON - START OF CORRIDOR\n",
    "# DID - DIDCOT PARKWAY - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# North Central Corridor\n",
    "# --------------------------------------------\n",
    "# BAN - BANBURY - START OF CORRIDOR\n",
    "# KGS - KINGS SUTTON\n",
    "# HYD - HEYFORD\n",
    "# TAC - TACKLEY\n",
    "# OXF - OXFORD\n",
    "# RAD - RADLEY\n",
    "# CUM - CULHAM\n",
    "# APF - APPLEFORD\n",
    "# DID - DIDCOT PARKWAY - END OF CORRIDOR\n",
    "\n",
    "# --------------------------------------------\n",
    "# Eastern Corridor\n",
    "# --------------------------------------------\n",
    "# DID - DIDCOT PARKWAY - START OF CORRIDOR\n",
    "# CHO - CHOLSEY\n",
    "# GOR - GORING AND STREATLEY\n",
    "# PAN - PANGBOURNE\n",
    "# TLH - TILEHURST\n",
    "# RDG - READING\n",
    "# TWY - TWYFORD\n",
    "# MAI - MAIDENHEAD\n",
    "# BNM - BURNHAM\n",
    "# SLO - SLOUGH\n",
    "# LNY - LANGLEY\n",
    "# IVR - IVER\n",
    "# WDT - WEST DRAYTON\n",
    "# HAY - HAYES AND HARLINGTON\n",
    "# STL - SOUTHALL\n",
    "# EAL - EALING BROADWAY\n",
    "# PAD - LONDON PADDINGTON - END OF CORRIDOR\n",
    "\n",
    "# used in defining the graph\n",
    "corridor_list = [\n",
    "    [\"SWA\", \"NTH\", \"PTA\", \"BGN\", \"CDF\", \"NWP\", \"BPW\", \"SWI\"],\n",
    "    [\"WSM\", \"WNM\", \"WOR\", \"YAT\", \"NLS\", \"BRI\", \"BTH\", \"CPM\", \"SWI\"],\n",
    "    [\"SWI\", \"DID\"],\n",
    "    [\"BAN\", \"KGS\", \"HYD\", \"TAC\", \"OXF\", \"RAD\", \"CUM\", \"APF\", \"DID\"],\n",
    "    [\"DID\", \"CHO\", \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"]\n",
    "]\n",
    "\n",
    "# rearrange sps with stops grouped by corridor\n",
    "sps_tmp = {}\n",
    "for corridor in corridor_list:\n",
    "    for j in corridor:\n",
    "        keys = list(sps_tmp)\n",
    "        if j not in keys:\n",
    "            sps_tmp[j] = sps[j]\n",
    "sps = sps_tmp\n",
    "\n",
    "# used in finding links between stops\n",
    "available_routes = [\n",
    "    [\"SWA\", \"NTH\", \"PTA\", \"BGN\", \"CDF\", \"NWP\", \"BPW\", \"SWI\", \"DID\", \"CHO\",\n",
    "     \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"],\n",
    "    \n",
    "    [\"WSM\", \"WNM\", \"WOR\", \"YAT\", \"NLS\", \"BRI\", \"BTH\", \n",
    "     \"CPM\", \"SWI\", \"DID\", \"CHO\", \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\",\n",
    "     \"BNM\", \"SLO\", \"LNY\", \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"],\n",
    "    \n",
    "    [\"BAN\", \"KGS\", \"HYD\", \"TAC\", \"OXF\", \"RAD\", \"CUM\", \"APF\", \"DID\", \"CHO\",\n",
    "     \"GOR\", \"PAN\", \"TLH\", \"RDG\", \"TWY\", \"MAI\", \"BNM\", \"SLO\", \"LNY\",\n",
    "     \"IVR\", \"WDT\", \"HAY\", \"STL\", \"EAL\", \"PAD\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:42.207135Z",
     "start_time": "2020-03-24T17:23:42.202380Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_links(stop_1, stop_2, available_routes):\n",
    "    # returns a list of all links between two stops on a route\n",
    "    # this relies on having an array of possible routes\n",
    "    #EX: my_list_1 = get_links(\"WDT\", \"PAD\")\n",
    "    # my_list_1 = [\"WDTHHAY\", \"HAYSTL\", \"STLEAL\", \"EALPAD\"]\n",
    "    done = 0\n",
    "    link_list = []\n",
    "    for i in range(len(available_routes)):\n",
    "        curr_route = available_routes[i]\n",
    "        if (stop_1 in curr_route) and (stop_2 in curr_route) and (not done):\n",
    "            done = 1\n",
    "            stop_1_idx = curr_route.index(stop_1)\n",
    "            stop_2_idx = curr_route.index(stop_2)            \n",
    "            for j in range(stop_1_idx, stop_2_idx):                \n",
    "                link_list.append(curr_route[j] + curr_route[j+1])\n",
    "        \n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:23:42.482542Z",
     "start_time": "2020-03-24T17:23:42.208601Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataframe setup\n",
    "\n",
    "# we're using scheduled times, but there is no reason that we couldn't use actual arrival/departure times\n",
    "## the reason we don't is b/c we would have scheduled tiems available in the real world prediction\n",
    "## maybe a better way would be to use scheduled times in the future predictions,\n",
    "## and actual times in for input data but this just adds needless complecation for a first stab at the problem\n",
    "\n",
    "# OD does not index the links, just the origin and destination of the route\n",
    "df[\"OD\"] = df[\"station_origin\"] + df[\"station_destination\"]\n",
    "\n",
    "# get integer time values for all rows (arrival and departure)\n",
    "df[\"dep_sched_int\"] = df[\"departure_sched\"]\n",
    "df[\"dep_sched_int\"][df[\"dep_sched_int\"] == \"terminating\"] = df[\"arrival_sched\"]\n",
    "df[\"dep_actual_int\"] = df[\"departure_actual\"]\n",
    "df[\"dep_actual_int\"][df[\"dep_actual_int\"] == \"terminating\"] = df[\"arrival_actual\"]\n",
    "\n",
    "# this one is different b/c \"starting\" arrival delay must be 0\n",
    "df[\"arr_sched_int\"] = df[\"arrival_sched\"]\n",
    "df[\"arr_sched_int\"][df[\"arr_sched_int\"] == \"starting\"] = df[\"departure_actual\"]\n",
    "df[\"arr_actual_int\"] = df[\"arrival_actual\"]\n",
    "df[\"arr_actual_int\"][df[\"arr_actual_int\"] == \"starting\"] = df[\"departure_actual\"]\n",
    "\n",
    "# get scheduled arrival and departure as datetime\n",
    "df[\"arr_sched_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"arr_sched_int\"].astype(str))\n",
    "df[\"dep_sched_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"dep_sched_int\"].astype(str))\n",
    "df[\"arr_actual_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"arr_actual_int\"].astype(str))\n",
    "df[\"dep_actual_datetime\"] = pd.to_datetime(df[\"date\"].astype(str) + \" \" + df[\"dep_actual_int\"].astype(str)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.910735Z",
     "start_time": "2020-03-24T17:23:42.483942Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the delay that was generated by the the previous section of track for each stop\n",
    "## arrival delay is positive if the train was late, and negative if the train was early\n",
    "\n",
    "# also get the departure time of the previous station (we need this to attribute the delay to the proper\n",
    "# time periods)\n",
    "\n",
    "# get the set of links each train travelled over between consecutive stops\n",
    "df[\"cum_arr_delay\"] = ((df[\"arr_actual_datetime\"] - df[\"arr_sched_datetime\"]).dt.total_seconds()) / 60\n",
    "init_list = []\n",
    "for i in range(len(df)):\n",
    "    init_list.append([])\n",
    "df[\"prev_links\"] = init_list\n",
    "\n",
    "\n",
    "# make sure we're only combining data from one train trip at a time\n",
    "for row_count in range(len(df)):\n",
    "    curr_row = df[row_count:row_count+1]\n",
    "    if (curr_row[\"arrival_sched\"].item() == \"starting\"):\n",
    "        df.at[row_count, \"prev_link_avg_datetime\"] = df.at[row_count, \"dep_sched_datetime\"]\n",
    "        df.at[row_count, \"prev_link_delay\"] = 0.\n",
    "        df.at[row_count, \"prev_links\"] = []\n",
    "        df.at[row_count, \"prev_station_dep_sched_datetime\"] = df.at[row_count, \"dep_sched_datetime\"]\n",
    "\n",
    "    else:\n",
    "        prev_row = df[row_count-1:row_count]\n",
    "\n",
    "        t0 = prev_row[\"dep_sched_datetime\"].item()\n",
    "        t1 = curr_row[\"arr_sched_datetime\"].item()\n",
    "\n",
    "        # average time at which the train was running on previous link\n",
    "        df.at[row_count, \"prev_link_avg_datetime\"] = t0 + ((t1-t0) / 2)\n",
    "        prev_links = get_links(prev_row[\"station_curr\"].item(), curr_row[\"station_curr\"].item(), available_routes)\n",
    "        df.at[row_count, \"prev_links\"] = prev_links\n",
    "        \n",
    "        # departure time from previous station\n",
    "        df.at[row_count, \"prev_station_dep_sched_datetime\"] = prev_row[\"dep_sched_datetime\"].item()\n",
    "\n",
    "        # average delay per link\n",
    "        # average the delay over # of links passed through by trains\n",
    "        ## Train 2: A -> D (doesn't stop at B or C but passes through)\n",
    "        ## 6 minute arrival delay at D\n",
    "        ## average delay of 6 minutes / 3 links = 2 minutes/link is attributed to each link b/c the data isn't\n",
    "        ## granular enough to provide a more accurate estimation\n",
    "        n_links = len(prev_links)\n",
    "        if (n_links == 0):\n",
    "            n_links = 1\n",
    "            \n",
    "        df.at[row_count, \"prev_link_delay\"] = (curr_row[\"cum_arr_delay\"].item() - prev_row[\"cum_arr_delay\"].item()) \\\n",
    "        / n_links\n",
    "        \n",
    "    row_count += 1\n",
    "        \n",
    "# alphabetize columns\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Rail Network Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.914580Z",
     "start_time": "2020-03-24T17:24:43.911901Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "G_stop2idx = {}\n",
    "G_idx2stop = {}\n",
    "for i in list(sps):\n",
    "    G_stop2idx[i] = counter\n",
    "    G_idx2stop[counter] = str(i)\n",
    "    counter += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.920746Z",
     "start_time": "2020-03-24T17:24:43.916139Z"
    }
   },
   "outputs": [],
   "source": [
    "# # processing to verify the corridors I defined\n",
    "# # Southwest Corridor\n",
    "# # check if any trains starting in TAU go through BPW or if they all go through SWI\n",
    "# ## Result: They all go through SWI, so the SW Corridor goes from TAU to SWI without branching off in this dataset\n",
    "# ## Result: The Northwest and Southwest Corridors only meet at SWI for this dataset\n",
    "\n",
    "# # North Central Corridor\n",
    "# ## all trains going from BAN to PAD must pass through DID to be part of this dataset\n",
    "# ## therefore there are no trains going from APF to CHO\n",
    "# ## no trains go from BAN and pass through SWI\n",
    "\n",
    "# df1 = df.copy()\n",
    "# df1 = df1.loc[df1[\"station_origin\"] == \"BAN\"]\n",
    "# df1 = df1.loc[df1[\"station_curr\"] == \"SWI\"]\n",
    "\n",
    "\n",
    "# df1.head()\n",
    "# print(len(df1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.927981Z",
     "start_time": "2020-03-24T17:24:43.921764Z"
    }
   },
   "outputs": [],
   "source": [
    "# create undirected graph representing inbound rail network\n",
    "\n",
    "total_str = \"\"\n",
    "for corridor in corridor_list:\n",
    "    for j in range(len(corridor)):\n",
    "        tmp_str = \"\"\n",
    "        if (j == 0):\n",
    "            tmp_str += str(G_stop2idx[corridor[j]])\n",
    "            tmp_str += \" \"\n",
    "            tmp_str += str(G_stop2idx[corridor[j+1]])\n",
    "\n",
    "        elif (j == len(corridor)-1):\n",
    "            tmp_str += str(G_stop2idx[corridor[j]])\n",
    "            tmp_str += \" \"\n",
    "            tmp_str += str(G_stop2idx[corridor[j-1]])\n",
    "\n",
    "        else:\n",
    "            tmp_str += str(G_stop2idx[corridor[j]])\n",
    "            tmp_str += \" \"\n",
    "            tmp_str += str(G_stop2idx[corridor[j-1]])\n",
    "            tmp_str += \" \"\n",
    "            tmp_str += str(G_stop2idx[corridor[j+1]])\n",
    "\n",
    "        total_str += tmp_str\n",
    "        total_str += \"\\n\"\n",
    "\n",
    "with open(adjlist_path, \"w\") as f:\n",
    "    f.write(total_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.934460Z",
     "start_time": "2020-03-24T17:24:43.929253Z"
    }
   },
   "outputs": [],
   "source": [
    "G_idx2location = {}\n",
    "counter = 0\n",
    "for i in G_stop2location:\n",
    "    G_idx2location[G_stop2idx[i]] = (G_stop2location[i][1], G_stop2location[i][0])\n",
    "\n",
    "G = nx.read_adjlist(adjlist_path, nodetype=int)\n",
    "LG = nx.line_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.942244Z",
     "start_time": "2020-03-24T17:24:43.936086Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the position of the edges as average of station positions\n",
    "LG_node2location = {}\n",
    "for edge in LG.nodes():\n",
    "    station_1 = int(edge[0])\n",
    "    station_2 = int(edge[1])\n",
    "    x1, x2 = G_idx2location[station_1][0], G_idx2location[station_2][0]\n",
    "    y1, y2 = G_idx2location[station_1][1], G_idx2location[station_2][1]\n",
    "    \n",
    "    x_edge = (x1+x2)/2\n",
    "    y_edge = (y1+y2)/2\n",
    "    LG_node2location[edge] = (x_edge, y_edge) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.947432Z",
     "start_time": "2020-03-24T17:24:43.943240Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get edge labels\n",
    "LG_node2label = {}\n",
    "for edge in LG.nodes():\n",
    "    station_1 = int(edge[0])\n",
    "    station_2 = int(edge[1])\n",
    "    \n",
    "    # i'm not sure why these two cases mess it up, but\n",
    "    # SWICPM should be CPMSWI\n",
    "    # DIDAPF should be APFDID\n",
    "    # something about the fact that they have multiple connections or are part of a complex junction\n",
    "\n",
    "    if (G_idx2stop[station_1] == \"SWI\") and (G_idx2stop[station_2] == \"CPM\") or \\\n",
    "        (G_idx2stop[station_1] == \"DID\") and (G_idx2stop[station_2] == \"APF\"):\n",
    "        station_tmp = station_1\n",
    "        station_1 = station_2\n",
    "        station_2 = station_tmp\n",
    "    \n",
    "    LG_node2label[edge] = G_idx2stop[station_1] + G_idx2stop[station_2]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:43.957568Z",
     "start_time": "2020-03-24T17:24:43.948519Z"
    }
   },
   "outputs": [],
   "source": [
    "# add self loops to graph\n",
    "for i in LG.nodes:\n",
    "    LG.add_edge(i, i)\n",
    "\n",
    "adj = nx.adjacency_matrix(LG).todense()\n",
    "adj = np.array(adj).astype(\"d\")\n",
    "np.save(adj_path, adj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Rail Link Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:44.118061Z",
     "start_time": "2020-03-24T17:24:43.958835Z"
    }
   },
   "outputs": [],
   "source": [
    "## find starting and ending time for the day\n",
    "def round_time(t_input, dt, direction=\"floor\"):\n",
    "    # \n",
    "    if direction == \"floor\":\n",
    "        n_minutes = ((pd.to_timedelta(str(t_input[0:2] + \":\" + t_input[2:4] + \":00\")).seconds / 60) // dt * dt)\n",
    "        return pd.to_timedelta(n_minutes, unit=\"m\")\n",
    "    \n",
    "    elif direction == \"ceil\":\n",
    "        n_minutes = ((pd.to_timedelta(str(t_input[0:2] + \":\" + t_input[2:4] + \":00\")).seconds / 60 + dt) // dt * dt)\n",
    "        return pd.to_timedelta(n_minutes, unit=\"m\")\n",
    "\n",
    "dt = 10 # minutes\n",
    "    \n",
    "t_min = min(df[\"dep_sched_int\"])\n",
    "t_max = max(df[\"dep_sched_int\"])\n",
    "t_start = round_time(t_min, dt)\n",
    "t_end = round_time(t_max, dt, \"ceil\")\n",
    "\n",
    "# unique times during each day\n",
    "time_list = []\n",
    "t_curr = t_start\n",
    "\n",
    "while t_curr < t_end:\n",
    "    time_list.append(t_curr)\n",
    "    t_curr += pd.Timedelta(str(dt) + \"minutes\")\n",
    "\n",
    "# unique dates of the year\n",
    "date_list = np.unique(df[\"date\"])\n",
    "\n",
    "datetime_list = []\n",
    "for i in date_list:\n",
    "    t0 = pd.to_datetime(i + \" 00:00:00\")\n",
    "        \n",
    "    for j in time_list:\n",
    "        t_curr = t0 + j\n",
    "        datetime_list.append(t_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:24:44.131558Z",
     "start_time": "2020-03-24T17:24:44.119190Z"
    }
   },
   "outputs": [],
   "source": [
    "## construct useful mappings\n",
    "# mapping from node description to unique index\n",
    "LG_node2idx = {}\n",
    "# mapping from unique index to node description \n",
    "LG_idx2node = {}\n",
    "counter = 0\n",
    "for i in LG.nodes():\n",
    "    LG_node2idx[i] = counter\n",
    "    LG_idx2node[counter] = i\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "# mapping from node description to label (i.e. \"SWAPAD\")\n",
    "LG_node_label = LG_node2label\n",
    "# mapping from label to node description\n",
    "LG_label2node = {}\n",
    "for i in LG_node_label:\n",
    "    LG_label2node[LG_node_label[i]] = i\n",
    "\n",
    "\n",
    "# mapping from time of year to time data index\n",
    "datetime2idx = {}\n",
    "# mapping from time data index to time of year\n",
    "idx2datetime = {}\n",
    "counter = 0\n",
    "for i in datetime_list:\n",
    "    datetime2idx[i] = counter\n",
    "    idx2datetime[counter] = i\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:26:17.942324Z",
     "start_time": "2020-03-24T17:24:44.132691Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_nodes = len(adj)\n",
    "n_timesteps = len(datetime_list)\n",
    "n_features = 1 #  delay per link\n",
    "n_timesteps_per_day = ((t_end - t_start)/dt).seconds / 60\n",
    "delay_dataset = np.zeros((n_timesteps, n_nodes, n_features))\n",
    "n_trains_total = np.zeros((n_nodes, 1))\n",
    "n_trains_link_dt = np.zeros((n_timesteps, n_nodes, 1))\n",
    "\n",
    "for i in range(len(datetime_list)-1):\n",
    "    # get set of trains running during current time period    \n",
    "    t0, t1 = datetime_list[i], datetime_list[i+1]\n",
    "    \n",
    "    # attribute the link delay to all relevant times \n",
    "    ## i.e. from scheduled departure of previous station to scheduled arrival at current station\n",
    "    t_avg_cond = ((t0 <= df[\"prev_link_avg_datetime\"]) & (df[\"prev_link_avg_datetime\"] <= t1))\n",
    "    t_arr_cond = ((t0 <= df[\"arr_sched_datetime\"]) & (df[\"arr_sched_datetime\"] <= t1))\n",
    "    t_dep_cond = ((t0 <= df[\"prev_station_dep_sched_datetime\"]) & (df[\"prev_station_dep_sched_datetime\"] <= t1))\n",
    "    \n",
    "    df_tmp = df.loc[t_avg_cond | t_dep_cond | t_arr_cond]\n",
    "    n_trains_link = np.zeros((n_nodes, 1))\n",
    "\n",
    "    for j in range(len(df_tmp)):\n",
    "        curr_row = df_tmp[j:j+1]\n",
    "        prev_links = curr_row[\"prev_links\"].item()\n",
    "\n",
    "        for k in range(len(prev_links)):\n",
    "            link = prev_links[k]\n",
    "            link_idx = LG_node2idx[LG_label2node[link]]\n",
    "\n",
    "            # sum the average delay experienced by trains passing through link during the time period\n",
    "            delay_dataset[i, link_idx, 0] += curr_row[\"prev_link_delay\"].item()\n",
    "            \n",
    "            # enumerate trains passing through each link during the time period\n",
    "            n_trains_link[link_idx] += 1\n",
    "    n_trains_link_dt[i, :, :] = n_trains_link\n",
    "    n_trains_total += n_trains_link\n",
    "    \n",
    "    # calculate delay over each link averaged by number of trains that passed through the link during the time period\n",
    "    divisor = n_trains_link + np.where(n_trains_link == 0, 1, 0)\n",
    "    delay_dataset[i, :, :] = delay_dataset[i, :, :] / divisor\n",
    "\n",
    "np.save(dataset_path, delay_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T17:26:17.946594Z",
     "start_time": "2020-03-24T17:26:17.943569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_timesteps_per_day: 42.0\n",
      "n_nodes 40\n"
     ]
    }
   ],
   "source": [
    "print(\"n_timesteps_per_day:\", n_timesteps_per_day)\n",
    "print(\"n_nodes\", n_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Visualization for ITSC Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:29.006515Z",
     "start_time": "2020-03-11T22:11:28.720632Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "\n",
    "plt.imshow(adj)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Number of traversals per link during 2016 and 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:29.292780Z",
     "start_time": "2020-03-11T22:11:29.007936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label2traversals = {}\n",
    "idx2traversals = {}\n",
    "\n",
    "for i in range(len(n_trains_total)):\n",
    "    label2traversals[LG_node_label[LG_idx2node[i]]] = int(n_trains_total[i].item())\n",
    "    idx2traversals[i] = int(n_trains_total[i].item())\n",
    "    \n",
    "idx2traversals = {k: v for k, v in sorted(idx2traversals.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "label2traversals = {k: v for k, v in sorted(label2traversals.items(), key=lambda item: item[1], reverse=True)}\n",
    "labels, y = zip(*label2traversals.items())\n",
    "labels = list(labels)\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = labels[i][0:3] + \"-\" + labels[i][3:]\n",
    "    \n",
    "x = np.arange(0, len(y))\n",
    "x = sorted(x, reverse=True)\n",
    "y = sorted(y, reverse=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.4, 4), dpi=dpi)\n",
    "plt.barh(x,y)\n",
    "# plt.ylabel(\"Link Name\")\n",
    "plt.xlabel(\"Attributed Link Traversals\", fontsize=6)\n",
    "plt.xticks(fontsize=5)\n",
    "plt.yticks(fontsize=5)\n",
    "ax.set(yticks=x, yticklabels=labels)\n",
    "plt.grid(axis=\"x\")\n",
    "# plt.title(\"Link Traversals for 2016 and 2017\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model Accuracy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:29.298205Z",
     "start_time": "2020-03-11T22:11:29.294037Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fp = \"./paper_data/experiment_results.csv\"\n",
    "df_acc = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:29.328203Z",
     "start_time": "2020-03-11T22:11:29.299676Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_names = np.unique(df_acc[\"Model\"])\n",
    "timesteps_in = np.unique(df_acc[\"Timesteps In\"])\n",
    "\n",
    "for i in model_names:\n",
    "    print(\"\\n\\n\", i)\n",
    "    df_tmp1 = df_acc.loc[df_acc[\"Model\"] == i]\n",
    "    for j in timesteps_in:\n",
    "        print(\"Timesteps In\", j)\n",
    "        df_tmp2 = df_tmp1.loc[df_tmp1[\"Timesteps In\"] == j]\n",
    "\n",
    "        print(\"MAE\")\n",
    "        mae_str = \"\"\n",
    "        for k in range(len(df_tmp2)):\n",
    "            curr_row = df_tmp2[k: k+1]\n",
    "            mae_str += str(round(curr_row[\"MAE (test)\"].item(), 3))\n",
    "            mae_str += \" / \"\n",
    "        print(mae_str)\n",
    "        \n",
    "        print(\"RMSE\")\n",
    "        rmse_str = \"\"\n",
    "        for k in range(len(df_tmp2)):\n",
    "            curr_row = df_tmp2[k: k+1]\n",
    "            rmse_str += str(round(curr_row[\"RMSE (test)\"].item(), 3))\n",
    "            rmse_str += \" / \"\n",
    "        print(rmse_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Plot of Graph and Line Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:29.998979Z",
     "start_time": "2020-03-11T22:11:29.329964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib import collections  as mc\n",
    "# fig, ax = plt.subplots(nrows=1 ,ncols=3,figsize=(7.5, 2.5), dpi=dpi)\n",
    "\n",
    "xlim = (-3.99, 0)\n",
    "ylim = (51.32, 52.08)\n",
    "\n",
    "fig = plt.figure(tight_layout=True, figsize=(7.05, 3.75), dpi=dpi)\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 0])\n",
    "nx.draw(G, G_idx2location, node_size=15, width=0.8, color=\"b\", edge_color=\"r\")\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.text(-4,52, \"(a)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(gs[0, 1])\n",
    "nx.draw(LG, LG_node2location, node_size=15, width=0.8, node_color=\"r\", edge_color=\"b\")\n",
    "# draw rectangle for the zoomed image\n",
    "rect = Rectangle((-0.8,51.44),0.7,0.15,linewidth=2,edgecolor='k',facecolor='None')\n",
    "ax.add_patch(rect)\n",
    "plt.text(-4,52, \"(b)\")\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.tight_layout()\n",
    "\n",
    "ax = fig.add_subplot(gs[1, :])\n",
    "plt.sca(ax)\n",
    "nx.draw(LG, LG_node2location, node_size=0, width=0.8, node_color=\"r\", edge_color=\"b\")\n",
    "nx.draw_networkx_labels(LG, LG_node2location, LG_node2label, font_size=8, font_weight=\"bold\")\n",
    "nx.draw_networkx_edges(LG, LG_node2location, edge_color=\"b\")\n",
    "\n",
    "# these work for zooming in around London\n",
    "xlim = (-0.71, -0.21)\n",
    "ylim = (51.503, 51.522)\n",
    "rect = Rectangle((-0.7091,51.5033),0.49,0.0186,linewidth=2,edgecolor='k',facecolor='None')\n",
    "ax.add_patch(rect)\n",
    "plt.text(-0.707,51.5033 + 0.0192, \"(c)\")\n",
    "\n",
    "# drawing line works, but it squishes the image vertically which isn't ideal for a paper\n",
    "# # draw lines pointing from the small rectangle to the large rectangle\n",
    "# # vertical line\n",
    "# # bottom point\n",
    "# x1, y1 = (-0.707+0.47, 51.503 + 0.0187), (-0.707+0.47, 51.503+0.0187)\n",
    "# # top point\n",
    "# x2, y2 = (-0.707+0.49, 51.503 + 0.037), (-0.707+0.49, 51.503+0.037)\n",
    "\n",
    "# # slanted line\n",
    "# #bottom point\n",
    "# x3, y3 = (-0.707+0.4, 51.503 + 0.0187), (-0.707+0.4, 51.503+0.0187)\n",
    "# # top point\n",
    "# x4, y4 = (-0.707+0.45, 51.503+0.037), (-0.707+0.45, 51.503+0.037)\n",
    "\n",
    "\n",
    "# lines = [[x1, y1, x2, y2], [x3, y3, x4, y4]]\n",
    "# # lines = [[x1, y1, x2, y2]]\n",
    "# lc = mc.LineCollection(lines, linewidths = 1, color=\"k\", zorder=0)\n",
    "# lc.set_clip_on(False)\n",
    "# ax.add_collection(lc)\n",
    "\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Other Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:30.003721Z",
     "start_time": "2020-03-11T22:11:30.001371Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# xlim, ylim based on min/max lat/long for the included stations in the dataset\n",
    "xlim = (-4.05, -0.05)\n",
    "ylim = (51.3, 52.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Visalize Graph and Line Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:30.206034Z",
     "start_time": "2020-03-11T22:11:30.004905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## graph with dots at station locations\n",
    "fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "nx.draw(G, G_idx2location, node_size=node_size, width=0.4)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:30.492916Z",
     "start_time": "2020-03-11T22:11:30.207247Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## graph with labels at station locations\n",
    "fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "nx.draw_networkx_labels(G, G_idx2location, G_idx2stop, font_size=3)\n",
    "nx.draw_networkx_edges(G, G_idx2location, width=0.1)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:30.683670Z",
     "start_time": "2020-03-11T22:11:30.493957Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## line graph with dots at link locations\n",
    "fig = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "nx.draw(LG, LG_node2location, node_size=node_size, width=0.4)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:30.972289Z",
     "start_time": "2020-03-11T22:11:30.684878Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## line grpah with labels at link locations\n",
    "fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "nx.draw_networkx_labels(LG, LG_node2location, LG_node2label, font_size=3)\n",
    "nx.draw_networkx_edges(LG, LG_node2location, width=0.1)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Actual delay vs predicted delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:32.313145Z",
     "start_time": "2020-03-11T22:11:30.973509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# first find a really busy time of the day\n",
    "total_trains_per_link = np.sum(n_trains_link_dt, axis=1)\n",
    "# 68 links were traversed on the entire network during this 10 minute period\n",
    "\n",
    "busy_idx = np.argmax(total_trains_per_link)\n",
    "curr_date = datetime_list[busy_idx]\n",
    "date_str = str(curr_date.year) + \"-0\" + str(curr_date.month) + \"-\" + str(curr_date.day)\n",
    "\n",
    "df_plot = df.loc[df[\"date\"] == date_str]\n",
    "\n",
    "# get time at which we will collect data\n",
    "datetime_plot = []\n",
    "t0 = pd.to_datetime(date_str + \" 00:00:00\")\n",
    "for j in time_list:\n",
    "    t_curr = t0 + j\n",
    "    datetime_plot.append(t_curr)\n",
    "    \n",
    "# get date indicies of delay_dataset for visualization of model \n",
    "idx_plot = []\n",
    "for i in datetime_plot:\n",
    "    if i in datetime_list:\n",
    "        idx_plot.append(datetime_list.index(i))\n",
    "\n",
    "datetime_plot_str = []\n",
    "for i in datetime_plot:\n",
    "    datetime_plot_str.append(str(i)[11:])\n",
    "data_plot = delay_dataset[idx_plot, :, :]\n",
    "fig, ax = plt.subplots(figsize=(16, 9), dpi=dpi)\n",
    "\n",
    "# plot the delay state for the n_plot busiest links during an average day\n",
    "x_ticks = np.arange(0, data_plot.shape[0])\n",
    "n_plot = 1\n",
    "counter = 0\n",
    "for i in list(idx2traversals):\n",
    "    if counter < n_plot:\n",
    "        plt.bar(x_ticks,data_plot[:, i].squeeze(), alpha=0.5)\n",
    "        plt.xticks(x_ticks)\n",
    "        ax.set_xticklabels(datetime_plot_str, rotation=90)\n",
    "        counter += 1\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:32.317255Z",
     "start_time": "2020-03-11T22:11:32.314490Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "in_list = [6, 12]\n",
    "out_list= [1, 3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:32.454019Z",
     "start_time": "2020-03-11T22:11:32.318978Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in in_list:\n",
    "    for j in out_list:\n",
    "        fp = \"./paper_data/training_curves/in_{}_out_{}_run-.-tag-Mean Absolute Error (testing).csv\".format(i, j)\n",
    "        df_train = pd.read_csv(fp)\n",
    "        plt.plot(df_train[\"Step\"], df_train[\"Value\"], label=(i, j))\n",
    "        plt.xlabel(\"Training Epoch\")\n",
    "        plt.ylabel(\"Test MAE\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:32.590444Z",
     "start_time": "2020-03-11T22:11:32.455351Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in in_list:\n",
    "    for j in out_list:\n",
    "        fp = \"./paper_data/training_curves/in_{}_out_{}_run-.-tag-Root Mean Squared Error (testing).csv\".format(i, j)\n",
    "        df_train = pd.read_csv(fp)\n",
    "        plt.plot(df_train[\"Step\"], df_train[\"Value\"], label=(i, j))\n",
    "        plt.xlabel(\"Training Epoch\")\n",
    "        plt.ylabel(\"Test RMSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T22:11:32.727533Z",
     "start_time": "2020-03-11T22:11:32.591514Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in in_list:\n",
    "    for j in out_list:\n",
    "        fp = \"./paper_data/training_curves/in_{}_out_{}_run-.-tag-Average Training Loss.csv\".format(i, j)\n",
    "        df_train = pd.read_csv(fp)\n",
    "        plt.plot(df_train[\"Step\"], df_train[\"Value\"], label=(i, j))\n",
    "        plt.xlabel(\"Training Epoch\")\n",
    "        plt.ylabel(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
